{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IGREbqWbU_4R"
   },
   "source": [
    "# MEP - Trabalho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ogr_43gocZ8v"
   },
   "source": [
    "## Resumo\n",
    "\n",
    "Blablabla\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Blablabla"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ILiNpAoUWRun"
   },
   "source": [
    "## Banco de dados\n",
    "\n",
    "Vamos utilizar o banco de dados da SCOPUS([1](https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus/)). Neste banco de dados, cada revista é classificada em uma área por especialistas humanos. Os artigos de cada revistam herdam a sua classificação de área. No momento de escrita deste artigo, a grande área de *Physical Sciences* contava com 115 subáreas. Para fins de exequibilidade, nos restringimos às 13 áreas relacionadas à Ciência da Computação e às 15 áreas relacionadas à Matemática:\n",
    "\n",
    "- 1700 - General Computer Science\n",
    "- 1701 - Computer Science (miscellaneous)\n",
    "- 1702 - Artificial Intelligence\n",
    "- 1703 - Computational Theory and Mathematics\n",
    "- 1704 - Computer Graphics and Computer-Aided Design\n",
    "- 1705 - Computer Networks and Communications\n",
    "- 1706 - Computer Science Applications\n",
    "- 1707 - Computer Vision and Pattern Recognition\n",
    "- 1708 - Hardware and Architecture\n",
    "- 1709 - Human-Computer Interaction\n",
    "- 1710 - Information Systems\n",
    "- 1711 - Signal Processing\n",
    "- 1712 - Software\n",
    "\n",
    "Por considerações de tempo, consideramos somente os tópicos de 1702 a 1706. A pesquisa avançada do próprio SCOPUS([2](https://www.scopus.com/search/form.uri?display=basic#basic)) nos permitiu obter resultados dentro de cada uma dessas classificações, excluindo-se aqueles que aparecem com mais de uma classificação, com strings de busca como\n",
    "\n",
    "    SUBJTERMS ( 1702 ) AND NOT ( SUBJTERMS ( 1703 ) OR SUBJTERMS ( 1704 ) OR SUBJTERMS ( 1705 ) OR SUBJTERMS ( 1706 ) )\n",
    "    \n",
    "O único metadado com o qual nos preocupamos são as palavras-chave.\n",
    "\n",
    "Consideramos os artigos com os maiores números de citações, que presumimos formarem bons representantes de diversas áreas do conhecimento (conforme a classificação humana). Uma alternativa seria considerar artigos conforme o parâmetro \"relevância\" da busca do SCOPUS, mas este parece sempre retornar artigos mais recentes.\n",
    "\n",
    " Nos restringimos a analisar somente as 2000 primeiras entradas de cada uma das áreas sob consideração. Excluindo entradas nas quais faltam informações, obtivemos aproximadamente 1000 entradas de cada área."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = []\n",
    "for i in range(5):\n",
    "    dados.append(pandas.read_csv(f\"170{i+2}.csv\").drop(labels=\"Link\",axis=1).dropna().reset_index(drop=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo nosso banco de dados em mãos, precisamos determinar um processo de análise de similaridade de entre palavras-chave e entre conjuntos dessas. Porém, também é comum que palavras-chave sejam compostas por mais de uma palavra, separadas por hífens e/ou espaços. Neste tipo de situação, separamos todas as palavras-chave compostas em suas constituintes. Também omitimos palavras com dois ou menos caracteres, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    dados[i][\"Author Keywords\"] = (dados[i][\"Author Keywords\"].str.lower()).str.split(r\"\\ |/|;|-|\\\\|,\",regex=True).apply(lambda l : [x for x in l if len(x)>3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deste modo, cada trabalho é identificado como o seu conjunto de palavras-chave. Nosso principal objetivo, agora, é elaborar uma métrica de similaridade entre conjuntos de palavras.\n",
    "\n",
    "Existem diversas técnicas para determinar similaridade entre palavras individuais. Técnicas sintáticas, como aquelas envolvendo Distância de Levenshtein e Similaridade de N-gramas, não são apropriadas para nossos fins, pois nosso foco é o conteúdo dos trabalhos. Neste trabalho, adotamos um modelo pré-treinado de Word2Vec disponível na biblioteca de Python Gensim. O modelo que escolhemos é o \"word2vec-google-news-300\", que é treinado em parte do conjunto de notícias do Google News, e conta com aproximadamente 3 milhões de frases e palavras. Este modelo permite que representemos palavras como vetores num espaço euclidiano 300-dimensional, e foi escolhido por sua grande abrangência.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTEhlZ0nDCIO",
    "outputId": "ec79e4b7-4c32-4018-f886-a054ed34220b"
   },
   "outputs": [],
   "source": [
    "# Para baixar o modelo pela primeira vez, utilize as próximas 2 linhas\n",
    "# import gensim.downloader as api\n",
    "# modelow2v = api.load('word2vec-google-news-300')\n",
    "#\n",
    "# Para salvar o modelo, utilize a próxima linha\n",
    "# modelow2v.save(\"gn300\")\n",
    "#\n",
    "# O modelo tem mais de ~1662.8MB, então vale a pena o salvar em disco para acesso mais rápido\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "modelow2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RhBXDuFGUX9b"
   },
   "outputs": [],
   "source": [
    "# modelow2v = KeyedVectors.load(\"gn300\", mmap='r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo problema a ser abordado é \"Como utilizar uma métrica de semelhança de palavras para determinar uma métrica de semelhança de conjuntos de palavras?\". Os conjuntos de palavras-chave não consistem de sentenças estruturadas com significado preciso; são apenas coleções de palavras que servem para resumir o assunto geral que é abordado no artigo. Por isso, modelos de assemelhação semântica de frases não são apropriados no nosso contexto.\n",
    "\n",
    "Este é um problema que não tem uma resposta bem-estabelecida, inclusive por seu escopo limitado (https://link.springer.com/chapter/10.1007/978-3-662-44415-3_20). Por isso, aplicamos uma abordagem genérica simples, nos utilizando do fato de que as palavras-chave individuais já estão representadas numericamente: Cada documento será representado, num espaço euclidiano 300-dimensional, pela média (normalizada sobre a esfera) de suas palavras-chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trabalhos_vetorizados = []\n",
    "\n",
    "for i in range(5):\n",
    "    trabalhos_vetorizados.append([])\n",
    "    for j in range(len(dados[i])):\n",
    "        trabalho_vetorizado = np.zeros(300)\n",
    "        for word in dados[i][\"Author Keywords\"][j]:\n",
    "            try:\n",
    "                trabalho_vetorizado += np.array(modelow2v[word])/np.linalg.norm(np.array(modelow2v[word]))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if np.linalg.norm(trabalho_vetorizado)==0:\n",
    "            # Um trabalho não pode ser vetorizado\n",
    "            pass\n",
    "        else:\n",
    "            trabalho_vetorizado = trabalho_vetorizado/np.linalg.norm(trabalho_vetorizado)\n",
    "            trabalhos_vetorizados[i].append(trabalho_vetorizado)\n",
    "        \n",
    "todos_trabalhos_vetorizados = []\n",
    "for i in range(5):\n",
    "    todos_trabalhos_vetorizados += trabalhos_vetorizados[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Isto permitirá aplicar algoritmos de clusterização sem necessidade de retrabalho. Para tal, escolhemos o algoritmo $K$-means clustering, disponível na biblioteca scikitlearn. Para completude, utilizamos outros métodos disponíveis nesta biblioteca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, BisectingKMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\", max_iter = 300).fit(todos_trabalhos_vetorizados)\n",
    "spcluster = SpectralClustering(n_clusters = 5).fit(todos_trabalhos_vetorizados)\n",
    "agcluster = AgglomerativeClustering(n_clusters = 5).fit(todos_trabalhos_vetorizados)\n",
    "bkmeans = BisectingKMeans(n_clusters = 5).fit(todos_trabalhos_vetorizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O cluster 0 tem 855 trabalhos\n",
      "O cluster 1 tem 1158 trabalhos\n",
      "O cluster 2 tem 1175 trabalhos\n",
      "O cluster 3 tem 1186 trabalhos\n",
      "O cluster 4 tem 1429 trabalhos\n"
     ]
    }
   ],
   "source": [
    "# Cria os clusteres após o treinamento\n",
    "clusteres_kmeans = []\n",
    "for i in range(5):\n",
    "    clusteres_kmeans.append([])\n",
    "for i in range(len(todos_trabalhos_vetorizados)):\n",
    "    clusteres_kmeans[kmeans.labels_[i]].append(todos_trabalhos_vetorizados[i])\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"O cluster {i} tem {len(clusteres_kmeans[i])} trabalhos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, calculamos os Índices de Davies-Bouldin padrão do clustering após o treinamento via $K$-means, e o comparamos com o índice do clustering antes do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denecessário\n",
    "\n",
    "def DB(clusteres , centroides = None):\n",
    "\n",
    "    n = len(clusteres)\n",
    "\n",
    "    if centroides is None:\n",
    "        centroides = []\n",
    "        for i in range(n):\n",
    "            # Calcula o centroide\n",
    "            centroide = np.sum(clusteres[i],axis=0)/len(clusteres[i])\n",
    "            centroides.append(centroide)\n",
    "\n",
    "\n",
    "    S = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        centroide = centroides[i]\n",
    "        S[i] = 1/len(clusteres[i]) * np.sum([np.linalg.norm(trabalho - centroide) for trabalho in clusteres[i]])\n",
    "\n",
    "    M = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    D = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i!= j:\n",
    "                M[i,j]= np.linalg.norm(centroides[i]-centroides[j])\n",
    "                R[i,j] = (S[i]+S[j])/M[i,j]\n",
    "                D[i] = max(D[i],R[i,j])\n",
    "    return (1/n)*np.sum(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O índice de Davies-Bouldin da clusterização original é 9.319913208305726\n"
     ]
    }
   ],
   "source": [
    "print(f\"O índice de Davies-Bouldin da clusterização original é {davies_bouldin_score(todos_trabalhos_vetorizados,[0]*len(trabalhos_vetorizados[0])+[1]*len(trabalhos_vetorizados[1])+[2]*len(trabalhos_vetorizados[2])+[3]*len(trabalhos_vetorizados[3])+[4]*len(trabalhos_vetorizados[4]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O índice de Davies-Bouldin da nova clusterização por K-Means é 4.470097311715916\n"
     ]
    }
   ],
   "source": [
    "print(f\"O índice de Davies-Bouldin da nova clusterização por K-Means é {davies_bouldin_score(todos_trabalhos_vetorizados,kmeans.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O índice de Davies-Bouldin da nova clusterização por Spectral Clustering é 4.423220794339837\n"
     ]
    }
   ],
   "source": [
    "print(f\"O índice de Davies-Bouldin da nova clusterização por Spectral Clustering é {davies_bouldin_score(todos_trabalhos_vetorizados,spcluster.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O índice de Davies-Bouldin da nova clusterização por Agglomerative Clustering é 5.454415182346748\n"
     ]
    }
   ],
   "source": [
    "print(f\"O índice de Davies-Bouldin da nova clusterização por Agglomerative Clustering é {davies_bouldin_score(todos_trabalhos_vetorizados,agcluster.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O índice de Davies-Bouldin da nova clusterização por Bisecting K-Means é 5.016578981587873\n"
     ]
    }
   ],
   "source": [
    "print(f\"O índice de Davies-Bouldin da nova clusterização por Bisecting K-Means é {davies_bouldin_score(todos_trabalhos_vetorizados,bkmeans.labels_)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O menor índice de Davies-Bouldin é o da nova clusterização por K-Means, o que indica que ela é melhor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações finais\n",
    "\n",
    "Utilizando técnicas bem-estabelecidas e facilmente acessíveis de aprendizado de máquinas, pudemos aprimorar - quantitativamente - a classificação humana sobre 5 subáreas do conhecimento de Ciência da Computação. Isso mostra que essa é uma direção promissora de trabalhos. Porém, trabalhamos em um ambiente bastante limitado\n",
    "\n",
    "Existem outras técnicas de análise semântica que poderiam ser adotadas (LSA sendo uma bem-conhecida), porém a praticidade de implementação de modelos de Word2Vec a fez preferível neste trabalho.\n",
    "\n",
    "Modelos baseados nas notícias do Google podem diferir significativamente de modelos treinados em grande bancos de trabalhos acadêmicos. Porém, não conseguimos encontrar modelos grande baseados em textos acadêmicos.\n",
    "\n",
    "Similarmente, o método de comparação de clusterings via índice de Davies-Boulding foi escolhido por ser apropriado ao modo com que trabalhamos com os dados.\n",
    "\n",
    "Por fim, a própria atribuição de palavras-chave aos trabalhos científicos pelos seus autores pode gerar o mesmo tipo de erro que o esperado nas classificações humanas dos campos de estudo de revistas científicas e artigos, que buscamos aprimorar. Uma análise mais efetiva poderia levar em consideração os textos completos. O acesso aos textos completos, por sua vez, é um fator impeditivo para tal análise, visto que as maiores editoras requerem assinaturas por parte dos leitores ou pagamento de taxas de *open access* por parte dos autores, ambas historicamente com valores proibitivos. Bases de dados abertas e amplamente utilizadas em certas linhas de pesquisa, tais como o [arXiv](arxiv.org), seriam uma poderosa ferramenta alternativa para possibilitar esse tipo de análise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z64tBB1XF6JM"
   },
   "source": [
    "Referências adicionais (não necessariamente botar na lista de referências!!)\n",
    "\n",
    "https://stackoverflow.com/questions/21979970/how-to-use-word2vec-to-calculate-the-similarity-distance-by-giving-2-words\n",
    "\n",
    "https://en.wikipedia.org/wiki/K-means_clustering\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "https://stackoverflow.com/questions/21979970/how-to-use-word2vec-to-calculate-the-similarity-distance-by-giving-2-words\n",
    "\n",
    "https://radimrehurek.com/gensim_3.8.3/sklearn_api/w2vmodel.html\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "filegpt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
